{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import abspath\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "pyspark_submit_args = \"--driver-memory 4g \"\n",
    "pyspark_submit_args += \"--executor-memory 4g \"\n",
    "pyspark_submit_args += \"pyspark-shell\"\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = pyspark_submit_args\n",
    "\n",
    "#warehouse_location = abspath('spark-warehouse')\n",
    "#spark = SparkSession.builder.config(\"spark.sql.warehouse.dir\", warehouse_location).appName(\"SparkSQL\").getOrCreate()\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log2row(line):    \n",
    "    fields = line.split(',')\n",
    "    row_eid = int(fields[0])\n",
    "    row_source = fields[2]\n",
    "    row_event = fields[3]\n",
    "    row_object = fields[4]\n",
    "    t = fields[1].split('T')\n",
    "    r_date = t[0].split('-')\n",
    "    r_time = t[1].split(':')\n",
    "    row_date = datetime.datetime(int(r_date[0]), int(r_date[1]), int(r_date[2]), 0, 0, 0)\n",
    "    row_time = datetime.datetime(1990, 1, 1, int(r_time[0]), int(r_time[1]), int(r_time[2]))\n",
    "    return Row(enrollment_id = row_eid, date = row_date, time = row_time, source = row_source, event = row_event, object_id = row_object )\n",
    "\n",
    "\n",
    "def true2row(line):\n",
    "    fields = line.split(',')\n",
    "    row_eid = int(fields[0])\n",
    "    row_dropout = bool(int(fields[1]))\n",
    "    return Row(enrollment_id = row_eid, dropout = row_dropout)\n",
    "        \n",
    "\n",
    "def date2row(line):\n",
    "    fields = line.split(',')\n",
    "    row_cid = fields[0]\n",
    "    r_from = fields[1].split('-')\n",
    "    r_to = fields[2].split('-')\n",
    "    row_from = datetime.datetime(int(r_from[0]), int(r_from[1]), int(r_from[2]), 0, 0, 0)\n",
    "    row_to = datetime.datetime(int(r_to[0]), int(r_to[1]), int(r_to[2]), 0, 0, 0)    \n",
    "    return Row(course_id = row_cid, fromdate = row_from, todate = row_to) \n",
    "\n",
    "def enrollment2row(line):\n",
    "    fields = line.split(',')\n",
    "    row_eid = int(fields[0])\n",
    "    row_username = fields[1]\n",
    "    row_cid = fields[2]\n",
    "    return Row(enrollment_id = row_eid, username = row_username, course_id = row_cid)\n",
    "  \n",
    "    \n",
    "def object2row(line):\n",
    "    fields = line.split(',')\n",
    "    if len(fields) == 5:    \n",
    "        row_cid = fields[0]\n",
    "        row_mid = fields[1]\n",
    "        row_category = fields[2]\n",
    "        row_children = fields[3]\n",
    "        row_date = datetime.datetime(1990, 1, 1, 0, 0, 0)\n",
    "        row_time = datetime.datetime(1990, 1, 1, 0, 0, 0)\n",
    "        if fields[4] != \"null\":\n",
    "            t = fields[4].split('T')\n",
    "            r_date = t[0].split('-')\n",
    "            r_time = t[1].split(':')\n",
    "            row_date = datetime.datetime(int(r_date[0]), int(r_date[1]), int(r_date[2]), 0, 0, 0)\n",
    "            row_time = datetime.datetime(1990, 1, 1, int(r_time[0]), int(r_time[1]), int(r_time[2]))\n",
    "        return Row(course_id = row_cid, module_id = row_mid, category = row_category, children=row_children, date=row_date, time=row_time)\n",
    "    elif len(fields) == 4:\n",
    "        row_cid = fields[0]\n",
    "        row_mid = fields[1]\n",
    "        row_category = fields[2]\n",
    "        row_children = \"\"        \n",
    "        row_date = datetime.datetime(1990, 1, 1, 0, 0, 0)\n",
    "        row_time = datetime.datetime(1990, 1, 1, 0, 0, 0)\n",
    "        if fields[3] != \"null\":\n",
    "            t = fields[3].split('T')\n",
    "            r_date = t[0].split('-')\n",
    "            r_time = t[1].split(':')\n",
    "            row_date = datetime.datetime(int(r_date[0]), int(r_date[1]), int(r_date[2]), 0, 0, 0)\n",
    "            row_time = datetime.datetime(1990, 1, 1, int(r_time[0]), int(r_time[1]), int(r_time[2]))\n",
    "        return Row(course_id = row_cid, module_id = row_mid, category = row_category, children=row_children, date=row_date, time=row_time)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert each csv file to RDD\n",
    "#Parse each line using function defined above\n",
    "#Convert parsed RDD to DataFrame and create temp views\n",
    "log_table = spark.sparkContext.textFile(\"Data/log.csv\")\n",
    "log_head = log_table.first()\n",
    "log_table_no_header = log_table.filter(lambda x : x != log_head)\n",
    "\n",
    "\n",
    "true_table = spark.sparkContext.textFile(\"Data/truth_train.csv\")\n",
    "true_head = true_table.first()\n",
    "true_table_no_header = true_table.filter(lambda x : x != true_head)\n",
    "\n",
    "\n",
    "date_table = spark.sparkContext.textFile(\"Data/date.csv\")\n",
    "date_head = date_table.first()\n",
    "date_table_no_header = date_table.filter(lambda x : x != date_head)\n",
    "\n",
    "\n",
    "enrollment_table = spark.sparkContext.textFile(\"Data/enrollment.csv\")\n",
    "enrollment_head = enrollment_table.first()\n",
    "enrollment_table_no_header = enrollment_table.filter(lambda x : x != enrollment_head)\n",
    "\n",
    "object_table = spark.sparkContext.textFile(\"Data/object.csv\")\n",
    "object_head = object_table.first()\n",
    "object_table_no_header = object_table.filter(lambda x : x != object_head)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "log_rows = log_table_no_header.map(log2row)\n",
    "true_rows = true_table_no_header.map(true2row)\n",
    "date_rows = date_table_no_header.map(date2row)\n",
    "enrollment_rows = enrollment_table_no_header.map(enrollment2row)\n",
    "object_rows = object_table_no_header.map(object2row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "log_df = spark.createDataFrame(log_rows)\n",
    "log_df.createOrReplaceTempView(\"log_t\")\n",
    "\n",
    "true_df = spark.createDataFrame(true_rows)\n",
    "true_df.createOrReplaceTempView(\"true_t\")\n",
    "\n",
    "date_df = spark.createDataFrame(date_rows)\n",
    "date_df.createOrReplaceTempView(\"date_t\")\n",
    "\n",
    "enrollment_df = spark.createDataFrame(enrollment_rows)\n",
    "enrollment_df.createOrReplaceTempView(\"enrollment_t\")\n",
    "\n",
    "object_df = spark.createDataFrame(object_rows)\n",
    "object_df.createOrReplaceTempView(\"object_t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------+--------+--------------------+------+-------------------+\n",
      "|               date|enrollment_id|   event|           object_id|source|               time|\n",
      "+-------------------+-------------+--------+--------------------+------+-------------------+\n",
      "|2014-06-14 00:00:00|            1|navigate|Oj6eQgzrdqBMlaCta...|server|1990-01-01 09:38:29|\n",
      "|2014-06-14 00:00:00|            1|  access|3T6XwoiMKgol57cm2...|server|1990-01-01 09:38:39|\n",
      "|2014-06-14 00:00:00|            1|  access|qxvBNYTfiRkNcCvM0...|server|1990-01-01 09:38:39|\n",
      "+-------------------+-------------+--------+--------------------+------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "root\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- enrollment_id: long (nullable = true)\n",
      " |-- event: string (nullable = true)\n",
      " |-- object_id: string (nullable = true)\n",
      " |-- source: string (nullable = true)\n",
      " |-- time: timestamp (nullable = true)\n",
      "\n",
      "+-------+-------------+\n",
      "|dropout|enrollment_id|\n",
      "+-------+-------------+\n",
      "|  false|            3|\n",
      "|  false|            4|\n",
      "|  false|            6|\n",
      "+-------+-------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+--------------------+-------------------+-------------------+\n",
      "|           course_id|           fromdate|             todate|\n",
      "+--------------------+-------------------+-------------------+\n",
      "|bWdj2GDclj5ofokWj...|2014-05-26 00:00:00|2014-06-24 00:00:00|\n",
      "|RXDvfPUBYFlVdlueB...|2014-05-25 00:00:00|2014-06-23 00:00:00|\n",
      "|fbPkOYLVPtPgIt0Mx...|2014-01-17 00:00:00|2014-02-15 00:00:00|\n",
      "+--------------------+-------------------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+--------------------+-------------+--------------------+\n",
      "|           course_id|enrollment_id|            username|\n",
      "+--------------------+-------------+--------------------+\n",
      "|DPnLzkJJqOOPRJfBx...|            1|9Uee7oEuuMmgPx2Iz...|\n",
      "|7GRhBDsirIGkRZBtS...|            3|1qXC7Fjbwp66GPQc6...|\n",
      "|DPnLzkJJqOOPRJfBx...|            4|FIHlppZyoq8muPbdV...|\n",
      "+--------------------+-------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+--------+--------------------+--------------------+-------------------+--------------------+-------------------+\n",
      "|category|            children|           course_id|               date|           module_id|               time|\n",
      "+--------+--------------------+--------------------+-------------------+--------------------+-------------------+\n",
      "|   about|                    |SpATywNh6bZuzm8s1...|1990-01-01 00:00:00|L1s4VseGlRT302GZl...|1990-01-01 00:00:00|\n",
      "|   about|                    |SpATywNh6bZuzm8s1...|1990-01-01 00:00:00|HxVne4dRqhXXf9FEs...|1990-01-01 00:00:00|\n",
      "| chapter|wq9HGmGdGoXFRgp4K...|SpATywNh6bZuzm8s1...|2014-08-11 00:00:00|3fpwAdewUyNZkLToS...|1990-01-01 01:00:00|\n",
      "+--------+--------------------+--------------------+-------------------+--------------------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_df.show(3)\n",
    "log_df.printSchema()\n",
    "true_df.show(3)\n",
    "date_df.show(3)\n",
    "enrollment_df.show(3)\n",
    "object_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL 1\n",
    "# COUNT: 7\n",
    "results1 = spark.sql('''SELECT enrollment_id,\n",
    "sum(CASE WHEN event = \"discussion\" THEN count_event ELSE 0 END) c_discusion,\n",
    "sum(CASE WHEN event = \"wiki\" THEN count_event ELSE 0 END) c_wiki,\n",
    "sum(CASE WHEN event = \"page_close\" THEN count_event ELSE 0 END) c_page_close,\n",
    "sum(CASE WHEN event = \"access\" THEN count_event ELSE 0 END) c_access,\n",
    "sum(CASE WHEN event = \"video\" THEN count_event ELSE 0 END) c_video,\n",
    "sum(CASE WHEN event = \"navigate\" THEN count_event ELSE 0 END) c_navigate,\n",
    "sum(CASE WHEN event = \"problem\" THEN count_event ELSE 0 END) c_problem \n",
    "FROM \n",
    " (SELECT enrollment_id, event, count(*) as count_event\n",
    " FROM log_t \n",
    " group by enrollment_id, event) \n",
    " group by enrollment_id \n",
    " order by enrollment_id ''').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL 2\n",
    "# COUNT: 2, ACCUMULATE: 9\n",
    "results2 = spark.sql('''SELECT enrollment_id,\n",
    "sum(CASE WHEN source = \"browser\" THEN count_event ELSE 0 END) s_browser,\n",
    "sum(CASE WHEN source = \"server\" THEN count_event ELSE 0 END) s_server\n",
    "FROM \n",
    " (SELECT enrollment_id,source, count(*) as count_event FROM log_t group by enrollment_id, source order by enrollment_id)\n",
    " group by enrollment_id\n",
    " order by enrollment_id\n",
    " ''').cache()\n",
    "joined_results1_2 = results2.join(results1,'enrollment_id','outer').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|           course_id|dropout_per_course|\n",
      "+--------------------+------------------+\n",
      "|1pvLqtotBsKv7QSOs...|              1383|\n",
      "|3VkHkmOtom3jM2wCu...|              1217|\n",
      "|3cnZpv6ReApmCaZya...|              1290|\n",
      "|5Gyp41oLVo7Gg7vF4...|              1930|\n",
      "|5X6FeZozNMgE2VRi3...|               603|\n",
      "+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#####################3 & 4 & 5\n",
    "\n",
    "# SQL 3\n",
    "# COUNT: 1, ACCUMULATE: 10\n",
    "results3 = spark.sql('SELECT course_id, count(enrollment_id) as count_eid_per_course FROM enrollment_t group by course_id order by course_id').cache()\n",
    "\n",
    "# SQL 4\n",
    "# COUNT: 1, ACCUMULATE: 11\n",
    "results4 = spark.sql('SELECT course_id, count(e.enrollment_id) as dropout_per_course FROM enrollment_t as e, true_t as t WHERE e.enrollment_id = t.enrollment_id and t.dropout = 1 group by course_id order by course_id').cache()\n",
    "\n",
    "results4.show(5)\n",
    "# SQL 5\n",
    "# COUNT: 1, ACCUMULATE: 12\n",
    "results5 = spark.sql('SELECT sq1.course_id as course_id, sq2.do/sq1.ce as dropoutrate_per_course FROM (SELECT course_id, count(enrollment_id) as ce FROM enrollment_t group by course_id) as sq1,(SELECT course_id, count(dropout) as do FROM enrollment_t as e, true_t as t WHERE e.enrollment_id = t.enrollment_id and t.dropout = 1 group by course_id) as sq2 WHERE sq1.course_id = sq2.course_id order by sq1.course_id').cache()\n",
    "\n",
    "\n",
    "joined_result_3_4_5 = results3.join(results4,'course_id','outer').join(results5,'course_id','outer').cache()\n",
    "\n",
    "joined_result_3_4_5_edf = joined_result_3_4_5.join(enrollment_df, 'course_id','outer').cache()\n",
    "joined_result1_2_3_4_5_edf = joined_result_3_4_5_edf.join(joined_results1_2, 'enrollment_id','outer').cache()\n",
    "\n",
    "##############################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################6 & 7 & 8\n",
    "\n",
    "#SQL 6\n",
    "# COUNT: 1, ACCUMULATE: 13\n",
    "results6 = spark.sql('SELECT username, count(enrollment_id) as count_eid_per_user FROM enrollment_t group by username order by username').cache()\n",
    "\n",
    "#SQL 7\n",
    "# COUNT: 1, ACCUMULATE: 14\n",
    "results7 = spark.sql('SELECT e.username as username, count(e.enrollment_id) as dropout_per_user FROM true_t as t, enrollment_t as e WHERE t.enrollment_id = e.enrollment_id and t.dropout = 1 group by e.username order by e.username').cache()\n",
    "\n",
    "# SQL 8\n",
    "# COUNT: 1, ACCUMULATE: 15\n",
    "results8 = spark.sql('SELECT sq1.username as username , sq2.do/sq1.ce as dropoutrate_per_user FROM (SELECT username, count(enrollment_id) as ce FROM enrollment_t group by username) as sq1,(SELECT username, count(dropout) as do FROM enrollment_t as e, true_t as t WHERE e.enrollment_id = t.enrollment_id and t.dropout = 1 group by username) as sq2 WHERE sq1.username = sq2.username order by sq1.username').cache()\n",
    "\n",
    "joined_result_6_7_8 = results6.join(results7,'username','outer').join(results8, 'username','outer').cache()\n",
    "joined_result1_2_3_4_5_6_7_8_edf = joined_result_6_7_8.join(joined_result1_2_3_4_5_edf, 'username','outer').drop('username').cache()\n",
    "\n",
    "##############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#SQL 9 \n",
    "# COUNT: 1, ACCUMULATE: 16\n",
    "results9 = spark.sql('SELECT e.enrollment_id, datediff( MAX(l.date), MIN(l.date) ) as period FROM log_t as l, enrollment_t  as e WHERE e.enrollment_id = l.enrollment_id group by e.enrollment_id order by e.enrollment_id').cache()\n",
    "\n",
    "joined_result1_2_3_4_5_6_7_8_9_edf = results9.join(joined_result1_2_3_4_5_6_7_8_edf, 'enrollment_id','outer').cache()\n",
    "\n",
    "\n",
    "\n",
    "# SQL 10\n",
    "# COUNT: 1, ACCUMULATE: 17\n",
    "results10 = spark.sql('SELECT  e.enrollment_id, count(distinct(l.date)) as effective_study_days FROM log_t as l, enrollment_t as e WHERE e.enrollment_id = l.enrollment_id group by e.enrollment_id order by e.enrollment_id').cache()\n",
    "\n",
    "joined_result1_2_3_4_5_6_7_8_9_10_edf = results10.join(joined_result1_2_3_4_5_6_7_8_9_edf, 'enrollment_id','outer').cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- enrollment_id: long (nullable = true)\n",
      " |-- effective_study_days: long (nullable = true)\n",
      " |-- period: integer (nullable = true)\n",
      " |-- count_eid_per_user: long (nullable = true)\n",
      " |-- dropout_per_user: long (nullable = true)\n",
      " |-- dropoutrate_per_user: double (nullable = true)\n",
      " |-- course_id: string (nullable = true)\n",
      " |-- count_eid_per_course: long (nullable = true)\n",
      " |-- dropout_per_course: long (nullable = true)\n",
      " |-- dropoutrate_per_course: double (nullable = true)\n",
      " |-- s_browser: long (nullable = true)\n",
      " |-- s_server: long (nullable = true)\n",
      " |-- c_discusion: long (nullable = true)\n",
      " |-- c_wiki: long (nullable = true)\n",
      " |-- c_page_close: long (nullable = true)\n",
      " |-- c_access: long (nullable = true)\n",
      " |-- c_video: long (nullable = true)\n",
      " |-- c_navigate: long (nullable = true)\n",
      " |-- c_problem: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_result1_2_3_4_5_6_7_8_9_10_edf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# SQL 11 subquery1\n",
    "# COUNT: 7, ACCUMULATE: 24\n",
    "results11 = spark.sql('''SELECT enrollment_id,\n",
    "sum(CASE WHEN event = \"discussion\" THEN count_event ELSE 0 END) c_complete_discusion,\n",
    "sum(CASE WHEN event = \"wiki\" THEN count_event ELSE 0 END) c_complete_wiki,\n",
    "sum(CASE WHEN event = \"page_close\" THEN count_event ELSE 0 END) c_complete_page_close,\n",
    "sum(CASE WHEN event = \"access\" THEN count_event ELSE 0 END) c_complete_access,\n",
    "sum(CASE WHEN event = \"video\" THEN count_event ELSE 0 END) c_complete_video,\n",
    "sum(CASE WHEN event = \"navigate\" THEN count_event ELSE 0 END) c_complete_navigate,\n",
    "sum(CASE WHEN event = \"problem\" THEN count_event ELSE 0 END) c_complete_problem \n",
    "FROM \n",
    " (SELECT e.course_id, e.enrollment_id, l.event, count(l.event) as count_event \n",
    " FROM log_t as l, enrollment_t as e, true_t as t \n",
    " WHERE t.dropout = 0 and t.enrollment_id = e.enrollment_id and e.enrollment_id = l.enrollment_id \n",
    " group by e.course_id, e.enrollment_id, l.event)\n",
    " group by enrollment_id\n",
    " order by enrollment_id\n",
    " ''').cache()\n",
    "\n",
    "joined_result1_2_3_4_5_6_7_8_9_10_11_edf = results11.join(joined_result1_2_3_4_5_6_7_8_9_10_edf, 'enrollment_id','outer').cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL 11 full query\n",
    "# COUNT: 7, ACCUMULATE: 31\n",
    "results12 = spark.sql('''SELECT sq2_cid as course_id,\n",
    "sum(CASE WHEN sq2_event = \"discussion\" THEN avg_c_complete_event ELSE 0 END) avg_c_complete_discusion,\n",
    "sum(CASE WHEN sq2_event = \"wiki\" THEN avg_c_complete_event ELSE 0 END) avg_c_complete_wiki,\n",
    "sum(CASE WHEN sq2_event = \"page_close\" THEN avg_c_complete_event ELSE 0 END) avg_c_complete_page_close,\n",
    "sum(CASE WHEN sq2_event = \"access\" THEN avg_c_complete_event ELSE 0 END) avg_c_complete_access,\n",
    "sum(CASE WHEN sq2_event = \"video\" THEN avg_c_complete_event ELSE 0 END) avg_c_complete_video,\n",
    "sum(CASE WHEN sq2_event = \"navigate\" THEN avg_c_complete_event ELSE 0 END) avg_c_complete_navigate,\n",
    "sum(CASE WHEN sq2_event = \"problem\" THEN avg_c_complete_event ELSE 0 END) avg_c_complete_problem \n",
    "FROM \n",
    " (SELECT sq1.c_id as sq2_cid, sq1.sq_event as sq2_event, avg(sq1.count_event) as avg_c_complete_event \n",
    "FROM (SELECT e.course_id as c_id, e.enrollment_id as e_id, l.event as sq_event, count(l.event) as count_event \n",
    "      FROM log_t as l, enrollment_t as e, true_t as t \n",
    "      WHERE t.dropout = 0 and t.enrollment_id = e.enrollment_id and e.enrollment_id = l.enrollment_id \n",
    "      group by e.course_id, e.enrollment_id, l.event) as sq1 \n",
    "group by sq1.c_id, sq1.sq_event)\n",
    "group by sq2_cid\n",
    "order by sq2_cid\n",
    "''').cache()\n",
    "\n",
    "joined_result1_2_3_4_5_6_7_8_9_10_11_12_edf = results12.join(joined_result1_2_3_4_5_6_7_8_9_10_11_edf, 'course_id','outer').drop('course_id').cache()\n",
    "\n",
    "\n",
    "joined_result_no_null = joined_result1_2_3_4_5_6_7_8_9_10_11_12_edf.na.fill(0)# fill null value in the dataframe as zero\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
